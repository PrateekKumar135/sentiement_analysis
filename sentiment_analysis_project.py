# -*- coding: utf-8 -*-
"""SENTIMENT ANALYSIS project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MuEKo9IdkqLbc6_uOQ2LenTmrrHbAAkc

# Read Data
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import nltk

amazon_data = pd.read_csv('/content/amazon_data.txt')

amazon_label = pd.read_csv('/content/amazon_label.txt')

data= pd.concat([amazon_data, amazon_label], axis=1)
 data.head()

"""# merged both files into a single & modified."""

# import pandas as pd

# Read the CSV file into a DataFrame
# df = pd.read_csv('your_dataset.csv')

# Define a dictionary to map old column names to new column names
column_name_mapping = {
    'canon 30d and 40d are way sexier might upgrade later after i build a nice lens collection': 'texts',
    'P': 'labels',
    # Add more mappings as needed
}

# Use the .rename() method to rename columns
data.rename(columns=column_name_mapping, inplace=True)

# Save the DataFrame back to a CSV file
data.to_csv('modified_dataset.csv', index=False)

data =pd.read_csv('/content/modified_dataset.csv')
data

"""# EDA"""

data['labels'].value_counts().plot(kind='bar',title='Count of Labels',xlabel='Labels')

"""# NLTK"""

pip install spacy

from nltk.corpus import stopwords
import string
import spacy

"""# Download NLTK resources"""

nltk.download('stopwords')
nltk.download('punkt')

"""# Load a spaCy language model"""

nlp = spacy.load("en_core_web_sm")

"""# Define a function for text preprocessing"""

def preprocess_text(text):
    # Tokenization with spaCy
    doc = nlp(text)

    # Lemmatization and lowercasing
    tokens = [token.lemma_.lower() for token in doc if not token.is_punct]

    # Remove stopwords
    tokens = [token for token in tokens if token not in stopwords.words('english')]

    # Remove numbers and special characters
    tokens = [token for token in tokens if not token.isdigit() and token not in string.punctuation]

    # Join the tokens back into a cleaned text
    cleaned_text = ' '.join(tokens)

    return cleaned_text

"""# Apply the preprocessing function to your dataset"""

cleaned_data = [preprocess_text(text) for text in data['texts']]
cleaned_data

nltk.download('punkt')

nltk.download('averaged_perceptron_tagger')

texts = str(cleaned_data)
tokens = nltk.word_tokenize(texts)
tokens[:5]

"""# PART OF SPEECH TAGGING"""

tagg = nltk.pos_tag(tokens)
tagg[:5]

"""# NLTK CHUNKING"""

nltk.download('maxent_ne_chunker')
nltk.download('words')

pip install svgling

entities = nltk.chunk.ne_chunk(tagg)
entities

"""# New section"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()

# Assuming 'preprocessed_data' is a list of preprocessed text data
tfidf_features = tfidf_vectorizer.fit_transform(cleaned_data)
tfidf_features

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Assuming 'tfidf_matrix' is your TF-IDF matrix
pca = PCA(n_components=20)
reduced_features = pca.fit_transform(tfidf_features.toarray())

# Create a scatter plot
plt.scatter(reduced_features[:, 0], reduced_features[:, 1], marker='o')
plt.title('PCA Visualization of TF-IDF')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import accuracy_score

nltk.download('vader_lexicon')

"""# Create a VADER sentiment analyzer"""

analyzer = SentimentIntensityAnalyzer()

# Transform the training and testing data using the vectorizer
X_train_tfidf = tfidf_vectorizer.transform(cleaned_data)
X_test_tfidf = tfidf_vectorizer.transform(cleaned_data)

"""# Create a sentiment classifier"""

classifier = SGDClassifier()

"""# Train the classifier on the training data."""

classifier.fit(X_train_tfidf, data['labels'])

"""# Predict the sentiment of the testing data."""

y_pred = classifier.predict(X_test_tfidf)
y_pred

"""# Evaluate the performance of the classifier."""

print(metrics.classification_report(data['labels'], y_pred))











